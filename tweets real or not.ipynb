{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import missingno\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,CategoricalNB\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"D:\\Projects\\spam\\train.csv\")\n",
    "test= pd.read_csv(r\"D:\\Projects\\spam\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove redundant samples\n",
    "train=train.drop_duplicates(subset=['text', 'target'], keep='first')\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop location and keyword column\n",
    "train = train.drop(['location','keyword'],axis=1)\n",
    "test = test.drop(['location','keyword'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify common words: Its the frequently used words as well as it could be potential data specific stop words.\n",
    "import pandas as pd\n",
    "\n",
    "freq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify uncommon words: Uncommon words in the train dataset\n",
    "freq1 =  pd.Series(' '.join(train ['text']).split()).value_counts()[-20:]\n",
    "freq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disaster tweet\n",
    "\n",
    "disaster_tweets = train[train['target'] ==1 ]['text']\n",
    "for i in range(1,10):\n",
    "    print(disaster_tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-disaster tweets\n",
    "non_disaster_tweets = train[train['target'] !=1 ]['text']\n",
    "non_disaster_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])\n",
    "wordcloud1 = WordCloud( background_color='white',\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(disaster_tweets))\n",
    "ax1.imshow(wordcloud1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Disaster Tweets',fontsize=20);\n",
    "\n",
    "\n",
    "wordcloud2 = WordCloud( background_color='white',\n",
    "                        width=600,\n",
    "                        height=400).generate(\" \".join(non_disaster_tweets))\n",
    "ax2.imshow(wordcloud2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Non Disaster Tweets',fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: clean_text(x))\n",
    "test['text'] = test['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "\n",
    "\n",
    "tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "train['text'] = train['text'].apply(lambda x:tokenizer.tokenize(x))\n",
    "test['text'] = test['text'].apply(lambda x:tokenizer.tokenize(x))\n",
    "train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words \n",
    "train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\n",
    "test['text'] = test['text'].apply(lambda x : remove_stopwords(x))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets = train[train['target'] ==1 ]['text']\n",
    "for i in range(1,10):\n",
    "    print(disaster_tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-disaster tweets\n",
    "non_disaster_tweets = train[train['target'] !=1 ]['text']\n",
    "non_disaster_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "lem = WordNetLemmatizer()\n",
    "def lem_word(x):\n",
    "    return [lem.lemmatize(w) for w in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lem_word)\n",
    "test['text'] = test['text'].apply(lem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(list_of_text):\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x : combine_text(x))\n",
    "test['text'] = test['text'].apply(lambda x : combine_text(x))\n",
    "train['text']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "\n",
    "nltk.download()\n",
    "#CountVectorizer\n",
    "word_vectorizer.fit_transform(text_issue_list.split('\\n'))\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_vector = count_vectorizer.fit_transform(train['text'])\n",
    "test_vector = count_vectorizer.transform(test['text'])\n",
    "print(train_vector[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 2,max_df = 0.5,ngram_range = (1,2))\n",
    "train_tfidf = tfidf.fit_transform(train['text'])\n",
    "test_tfidf = tfidf.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha = 2.0)\n",
    "scores_vector = model_selection.cross_val_score(mnb,train_vector,train['target'],cv = 10,scoring = 'f1')\n",
    "print(\"score:\",scores_vector)\n",
    "scores_tfidf = model_selection.cross_val_score(mnb,train_tfidf,train['target'],cv = 10,scoring = 'f1')\n",
    "print(\"score of tfidf:\",scores_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(C = 1.0)\n",
    "scores_vector = model_selection.cross_val_score(lg, train_vector, train[\"target\"], cv = 5, scoring = \"f1\")\n",
    "print(\"score:\",scores_vector)\n",
    "scores_tfidf = model_selection.cross_val_score(lg, train_tfidf, train[\"target\"], cv = 5, scoring = \"f1\")\n",
    "print(\"score of tfidf:\",scores_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(train_tfidf, train[\"target\"])\n",
    "y_pred = mnb.predict(test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = input(Enter a tweet)\n",
    "prediction = mnb.predict(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
