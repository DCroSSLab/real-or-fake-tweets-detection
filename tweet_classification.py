# -*- coding: utf-8 -*-
"""Tweet Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JQSPqO5xE8HGdFkfksaO4xbrg0q28tIM

### Importing libraries
"""

import pandas as pd
import numpy as np

import re
import string
import os

import matplotlib.pyplot as plt
import seaborn as sns

from wordcloud import WordCloud

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

plt.style.use('ggplot')

from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout
from tensorflow.keras.optimizers import Adam

"""### Downloading Dataset"""

tweet = pd.read_csv("https://raw.githubusercontent.com/laxmimerit/twitter-disaster-prediction-dataset/master/train.csv")

tweet.head()

tweet.shape

tweet.info()

"""### Exploratory Data Analysis

### Target Class Distribution
"""

plt.rcParams['figure.figsize'] = [8,4]
plt.rcParams['figure.dpi'] = 100

sns.countplot('target', data = tweet)
plt.title('Real or Fake Disaster Tweet')

tweet['target'].value_counts()

tweet['target'].value_counts().plot.pie(autopct='%1.2f%%')

"""### Numbers of character Distribution in Tweets"""

!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-reinstall

import preprocess_kgptalkie as kgp

tweet = kgp.get_basic_features(tweet)

tweet.head()

sns.distplot(tweet['char_counts'])

sns.kdeplot(tweet[tweet['target']==1]['char_counts'],shade = True)
sns.kdeplot(tweet[tweet['target']==0]['char_counts'],shade = True)

sns.catplot(y = 'char_counts', data = tweet, kind= 'violin', col = 'target')

"""### Number of Words, Average Words Length, and Stop Words Distribution in tweets"""

tweet

sns.kdeplot(tweet[tweet['target']==1]['word_counts'],shade = True, color = 'green')
sns.kdeplot(tweet[tweet['target']==0]['word_counts'],shade = True, color = 'red')

sns.kdeplot(tweet[tweet['target']==1]['avg_wordlength'],shade = True, color = 'green')
sns.kdeplot(tweet[tweet['target']==0]['avg_wordlength'],shade = True, color = 'red')

sns.kdeplot(tweet[tweet['target']==1]['stopwords_counts'],shade = True, color = 'green')
sns.kdeplot(tweet[tweet['target']==0]['stopwords_counts'],shade = True, color = 'red')

"""### Most and Least Words"""

freqs = kgp.get_word_freqs(tweet,'text')

top20 = freqs[:20]

plt.bar(top20.index, top20.values)

least20 = freqs[-20:]
least20

bigram = kgp.get_ngram(tweet, 'text', ngram_range=2)
bigram

bigram[:20]

bigram[-20:]

"""### One Shot Data Cleaning"""

def get_clean(x):
    x = str(x).lower().replace('\\', '').replace('_', ' ').replace('.', ' ')
    x = kgp.cont_exp(x)
    x = kgp.remove_emails(x)
    x = kgp.remove_urls(x)
    x = kgp.remove_html_tags(x)
    x = kgp.remove_rt(x)
    x = kgp.remove_accented_chars(x)
    x = kgp.remove_special_chars(x)
    x = kgp.remove_dups_char(x)
    return x

tweet['text'] = tweet['text'].apply(lambda x: get_clean(x))

tweet.head()['text']

"""## Disaster Words Visualisation with Word Cloud"""

real = kgp.get_word_freqs(tweet[tweet['target']==1], 'text')
real = ' '.join(real.index)
real

word_cloud = WordCloud(max_font_size=100).generate(real)
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

fake = kgp.get_word_freqs(tweet[tweet['target']==0], 'text')
fake = ' '.join(fake.index)
fake
word_cloud = WordCloud(max_font_size=100).generate(fake)
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

"""## Classification with TFIDF and SVM"""

text = tweet['text']
y = tweet['target']

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(text)

X.shape

X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size = 0.2, random_state=0, stratify = y)

def run_SVM(clf,X_train, X_test, y_train, y_test ):
  clf.fit(X_train,y_train)
  y_pred = clf.predict(X_test)

  print()
  print('Classification Report')
  print(classification_report(y_test, y_pred))

from sklearn.svm import LinearSVC

clf = LinearSVC()
run_SVM(clf, X_train, X_test, y_train,  y_test )

"""### Classification with Word2vec and SVM"""

!python -m spacy download en_core_web_lg

import spacy
import en_core_web_lg

nlp = en_core_web_lg.load()

x = 'cat dog'
doc = nlp(x)

doc.vector.shape

def get_vec(x):
  doc = nlp(x)
  vec = doc.vector
  return vec

tweet['vec'] = tweet['text'].apply(lambda x: get_vec(x))

tweet.head()

X = tweet['vec'].to_numpy()
X = X.reshape(-1,1)

X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, 300)

X.shape

X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size = 0.2, random_state=0, stratify = y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# clf = LinearSVC()
# run_SVM(clf, X_train, X_test, y_train,  y_test )

"""### Word Embeddings and Classification with Deep Learning"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout
from tensorflow.keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D

token = Tokenizer()
token.fit_on_texts(text)

vocab_size = len(token.word_index) + 1
vocab_size

print(token.word_index)

encoded_text = token.texts_to_sequences(text)

encoded_text = token.texts_to_sequences(text)

print(encoded_text)

max_len = 40
X = pad_sequences(encoded_text, maxlen=max_len, padding='post')

print(X)

X.shape

X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size = 0.2, random_state=0, stratify = y)

vec_size = 100

model = Sequential()
model.add(Embedding(vocab_size, vec_size, input_length=max_len))

model.add(Conv1D(32,2, activation='relu'))
model.add(MaxPool1D(2))
model.add(Dropout(0.5))

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(16, activation='relu'))

model.add(GlobalMaxPooling1D())

model.add(Dense(1, activation='sigmoid'))

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# model.fit(X_train, y_train, epochs = 5, validation_data=(X_test, y_test))

def get_encoded(x):
  x = get_clean(x)
  x = token.texts_to_sequences([x])
  x = pad_sequences(x, maxlen = max_len, padding = 'post')
  return x

x = 'I am a girl'
vec = get_encoded(x)

np.argmax(model.predict(vec), axis = -1)

"""### Bert Model Building and Training"""

!pip install ktrain
from ktrain import text
import ktrain

(X_train, y_train), (X_test, y_test), preproc= text.texts_from_df(train_df=tweet, text_column= 'text', label_columns= 'target',  maxlen= 40, preprocess_mode= 'bert')

model = text.text_classifier(name = 'bert', train_data= (X_train, y_train), preproc= preproc)

learner = ktrain.get_learner(model = model, train_data= (X_train, y_train), val_data= (X_test, y_test), batch_size= 64)

learner.fit_onecycle(lr = 2e-5, epochs = 1)

learner.fit_onecycle(lr = 2e-5, epochs = 5)

predictor = ktrain.get_predictor(learner.model, preproc)

data = [' I met you today by accident', 'I got a car accident, I am injured']

predictor.predict(data[1])

